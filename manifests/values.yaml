# Default airflow tag to deploy
defaultAirflowTag1: "3.0.1"
fernetKey: 'fsdfsfsf='

airflow:
  image:
    repository: apache/airflow
    tag: 3.0.1
    pullPolicy: Always

  config:
    AIRFLOW__CORE__EXECUTOR:
      value: CeleryExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN:
      value: postgresql+psycopg2://airflow:airflow@postgresql:5432/airflow
    AIRFLOW__CORE__FERNET_KEY:
      value: ""
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION:
      value: "true"
    AIRFLOW__API__AUTH_BACKEND:
      value: airflow.api.auth.backend.basic_auth

  users:
    - username: admin
      password: admin
      role: Admin
      email: admin@example.com
      firstName: Admin
      lastName: User

  extraEnv: ""  # Must be null or string if not using complex env logic

  webserver:
    defaultUser:
      enabled: true
      email: admin@example.com
      username: admin
      password: admin
      firstName: Admin
      lastName: User
    service:
      type: ClusterIP  # Remove `port`, it's handled by chart

  scheduler:
    replicas: 1

  workers:
    replicas: 2  # Do not set `enabled`

  triggerer:
    replicas: 1

  logs:
    persistence:
      enabled: true
      size: 1Gi
      storageClassName: standard

postgresql:
  enabled: true
  auth:
    username: airflow
    password: airflow
    database: airflow
  primary:
    persistence:
      enabled: true
      size: 1Gi
      storageClass: standard

flower:
  enabled: true
  service:
    type: ClusterIP

redis:
  enabled: true

dags:
  persistence:
    enabled: true
    existingClaim: airflow-dags-pvc
    subPath: dags

createUserJob:
  useHelmHooks: false
  applyCustomEnv: false

migrateDatabaseJob:
  useHelmHooks: false
  applyCustomEnv: false
  jobAnnotations:
    "argocd.argoproj.io/hook": Sync


ingress:
  enabled: true
  web:
    annotations:
      kubernetes.io/ingress.class: nginx
    hosts:
      - host: airflow.example.com
        paths:
          - path: /
            pathType: Prefix
    tls: false  # or provide array of objects, not a map
